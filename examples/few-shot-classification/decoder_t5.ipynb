{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646946a9-e838-4c1c-bd3c-3ae1d2f4a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68777029-7c4e-4188-b2f5-9dcdb9a807bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azon/anaconda3/envs/ml/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-3b\", padding_side='left')\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2b8748-e3df-4ed2-9514-2bbcaad72095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5611a937-b119-43de-9868-7598ccde5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer([\"Hello, my dog is cute\", \"Hello\"], return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfb4bc8-2d85-4b47-a8ba-81a4511c2113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[8774,    6,   82, 1782,   19, 5295,    1],\n",
       "        [   1,    1,    1,    1,    1, 8774,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e1beb8-b5f9-46d8-9afb-c6edec452e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 32128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e5cd4cd-63ea-4099-8bc5-0543d2524366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db713c25-c86b-4833-8aa8-ac0a3e521748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1377d535-78a6-43d8-aaad-5b97b24619c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=256, num_beams=5, early_stopping=True, no_repeat_ngram_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a2f944-bd62-41a1-ac59-063592c1c6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     3,     6,    82,  1782,     6, 32094,     6, 32093,     6,\n",
       "         32092,     6, 32091,     6, 32090,     6,     8,  1782,    19,     3,\n",
       "             9,  1782,     5,    27,   183,  1782,     3,    18,    82,  3947,\n",
       "          1782,    55,     6,    27,  1782,    18, 10169,     6,     3,    88,\n",
       "             3,   233,  8774,     6, 32089,     6, 21820,     6, 32088,    55,\n",
       "             3,     5,     3,     2,     3,     7,  1782, 32090,     5, 32090,\n",
       "            55,    27,     3,    17,     3,    15,  1782,    11,    82, 17351,\n",
       "             5, 32086,     5,     6,     6,    11,  1782,  1782,    82,     6,\n",
       "          1782, 17351,     6,    69,  1782, 32086,     6, 32085,     6, 32084,\n",
       "             6, 32083,     6,     5,     5,    82,  3887,    19,     6, 32079,\n",
       "             6, 32096,     6, 32095,     6,     1],\n",
       "        [    0, 32099,     5, 32098,  8774,     5,  8774, 21820, 21820,     5,\n",
       "         32097,     5, 32096,     5, 32095,     5, 32094,     5, 32093,     5,\n",
       "         32092,     5, 32091,     5, 32090,     5, 32089,     3, 32088,     3,\n",
       "         32087,     3, 32086,     3, 32085,     3, 32084,     3, 32083,  8774,\n",
       "            55,  8774,  8774,     3, 13133, 21820,     3,   107,    76,     9,\n",
       "          1123,    23,     3,   226,  2689,  9181,     3,   102,     7,   591,\n",
       "             3,   104,     3,    23,  5612,    58,     3,    58,   107,  7126,\n",
       "         21820,  7102, 21820,    55,     3,     2,     3,     9,   107,   107,\n",
       "         24303,    55,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d1d892-cea1-4338-9e2e-9657447a4c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", my dog,,,,,, the dog is a dog. I am dog - my pet dog!, I dog-dog, he... Hello,, hello,!.  s dog.! I t e dog and my puppy..,, and dog dog my, dog puppy, our dog,,,,.. my dogs is,,,,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(infer[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ab21979-f9d0-40c8-9180-21f054b0cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Hello. Hello hello hello.........       Hello! Hello Hello hey hello huawei xbox 360 ps4 – ipad??hello hello hi hello!  ahhhhh!\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(infer[1], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ed08f2d-de48-4183-b05c-c574eae90a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer2 = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=256, num_beams=5, early_stopping=True, no_repeat_ngram_size=2, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e12a5987-bd45-4a5d-967b-7527f20fda0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 106])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffdfafbf-f70b-4565-9a3f-ce57e459fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", my dog,,,,,, the dog is a dog. I am dog - my pet dog!, I dog-dog, he... Hello,, hello,!.  s dog.! I t e dog and my puppy..,, and dog dog my, dog puppy, our dog,,,,.. my dogs is,,,,\n",
      ", my dog,,,,,, the dog is a dog. I am dog - my pet dog!, I dog-dog, he... Hello,, hello,!.  s dog.! I t e dog and my puppy..,, and dog dog my, dog puppy, our dog,,,,.. my dogs is..,,\n",
      ", my dog,,,,,, the dog is a dog. I am dog - my pet dog!, I dog-dog, he... Hello,, hello,!.  s dog.! I t e dog and my puppy..,, and dog dog my, dog puppy, our dog,,,,.. my dogs is.,,,\n",
      ", my dog,,,,,, the dog is a dog. I am dog - my pet dog!, I dog-dog, he... Hello,, hello,!.  s dog.! I t e dog and my puppy..,, and dog dog my, dog puppy, our dog,,,,.. my dogs is.., pet,\n",
      ", my dog,,,,,, the dog is a dog. I am dog - my pet dog!, I dog-dog, he... Hello,, hello,!.  s dog.! I t e dog and my puppy..,, and dog dog my, dog puppy, our dog,,,,.. my dogs is..,.\n",
      ". Hello. Hello hello hello.........       Hello! Hello Hello hey hello huawei xbox 360 ps4 – ipad??hello hello hi hello!  ahhhhh!\n",
      ". Hello. Hello hello hello.........       Hello! Hello Hello hey hello huawei xbox 360 ps4 – ipad??hello hello hi hello!  ahhhhh.\n",
      ". Hello. Hello hello hello.........       Hello! Hello Hello hey hello huawei xbox 360 ps4 – ipad??hello hello hi hello!  ahhhhh..\n",
      ". Hello. Hello hello hello.........       Hello! Hello Hello hey hello huawei xbox 360 ps4 – ipad??hello hello hi hello!  ahhhhh? hello? Hello hi!\n",
      ". Hello. Hello hello hello.........       Hello! Hello Hello hey hello huawei xbox 360 ps4 – ipad??hello hello hi hello!  ahhhhh......\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.decode(infer2[i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "498a7abd-b559-431d-8e61-339da494e7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f56994291b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bc3a73e-2f04-432c-b803-d4ef21ab417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0,\n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2beb9044-2684-43b8-a69a-d15ab289f459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      ", my dog from the dog, my dog, my dog, my dog is,,,,, my dog is my dog, my dog a dog, my dog, my dog, our dog, my\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea1516-d7c4-4fa0-885f-0c03764826dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e023fb-93a3-4858-b34b-f6f6bebe5e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd823fd7-c2fa-4f09-863b-36f33e9c0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=6,\n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be055925-8e8b-4553-a038-2fd29141df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(tokenizer.decode(sample_output[i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4924462-547e-466e-8753-24fa43fe060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    "    top_k=0,\n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54946b39-08e8-472f-b220-563c43d2354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(tokenizer.decode(sample_output[i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d6bbf-06e9-4245-ab0d-df8a7039c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.95, \n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adf5a4-b889-4b63-b59d-5a18f3c19bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    print(tokenizer.decode(sample_output[i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f1068-a42d-4bfc-a15b-9e896c6f0b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
